# Sabine 2.0 Implementation Plan (Cost-Optimized, Parallel Execution)

> **For Claude:** REQUIRED SUB-SKILL: Use `superpowers:dispatching-parallel-agents` for Phase 0 ADRs, then `superpowers:subagent-driven-development` for Phase 1-4 execution.

**Goal:** Transform Sabine from a reactive assistant into a proactive strategic partner with memory-based reasoning, conflict resolution, and autonomous skill acquisitionâ€”within 16 weeks while minimizing token costs and human-in-loop time.

**ðŸ”’ CRITICAL: Zero-Downtime Development**
- Sabine 1.0 code is UNTOUCHED during entire 16-week project
- Sabine 2.0 built in `/backend/v2/` (isolated, no conflicts)
- Feature flags allow instant rollback (kill switches)
- Regression tests verify 1.0 unchanged after every commit
- See "Backward Compatibility Strategy" section below

**Architecture:**
- **Dual-stream pipeline:** Fast Path (10-12s latency) queues work, Slow Path (async worker) consolidates nightly
- **MAGMA graph storage:** Relationship extraction from memories for causal tracing and semantic understanding
- **Belief system:** Non-monotonic memory revision with configurable confidence thresholds
- **Autonomy layer:** Gap detection â†’ E2B sandbox testing â†’ user-approved skill promotion
- Token optimization: Haiku-only for extraction, caching for repetitive patterns, batch processing in Slow Path

**Tech Stack:**
- Storage: Supabase PostgreSQL + pgvector (vector DB)
- Job Queue: Redis + rq (isolated worker processes)
- Sandbox: E2B (AI-focused, cold-start ~2s)
- LLM: Claude Haiku (cost + speed optimized)
- Monitoring: Grafana + telemetry tracing

---

## Backward Compatibility Strategy

### Sabine 1.0 Zero-Downtime Guarantee

**Sabine 1.0 code is 100% untouched:**
```
/lib/agent/memory.py      âœ“ UNCHANGED
/lib/agent/retrieval.py   âœ“ UNCHANGED
/lib/agent/core.py        âœ“ UNCHANGED
```

**If Sabine 2.0 breaks:** Set `ENABLE_SABINE_V2=false` â†’ rollback in <1 minute

### Isolated Architecture
```
FastAPI Server (1.0)              â† Unchanged, production-critical
â”œâ”€ Writes to memories table       â† Unchanged
â””â”€ Also writes to WAL table (v2)  â† Non-blocking, new code

Separate Worker Process (2.0)     â† Isolated, different Railway service
â”œâ”€ Reads WAL table
â”œâ”€ Writes to entity_relationships
â”œâ”€ Computes salience_scores
â””â”€ Archives cold memories
```

If worker crashes: Sabine 1.0 completely unaffected (different process).

### Schema Changes (Additive Only)

**New columns with defaults + NULL (safe to add):**
```sql
ALTER TABLE memories ADD COLUMN salience_score FLOAT DEFAULT 1.0;
ALTER TABLE memories ADD COLUMN is_archived BOOLEAN DEFAULT false;
ALTER TABLE memories ADD COLUMN confidence FLOAT DEFAULT 1.0;
```

**Sabine 1.0 queries still work:** `SELECT * FROM memories` âœ“

**New tables (zero impact on 1.0):**
```sql
CREATE TABLE write_ahead_log (...);      -- 1.0 never reads
CREATE TABLE entity_relationships (...); -- 1.0 never reads
```

### Dual-Write (Fast Path)

Every user message written to BOTH:
- `memories` table (Sabine 1.0 reads this)
- `write_ahead_log` table (Sabine 2.0 reads this)

Non-blocking: if WAL write fails, user response still sent immediately.

### Canary Rollout

- **Weeks 1-3:** Code built, not running (dark mode)
- **Week 4:** Manual testing only (dev/staging)
- **Week 5+:** Scheduled at 2:00 AM (shadow mode, no 1.0 impact)
- **Week 8+:** Optional migration to 2.0 retrieval (when validated)

### Rollback Plan

```bash
# If 2.0 breaks:
ENABLE_SABINE_V2=false   # Kill switch

# Results:
# âœ“ Sabine 1.0 continues untouched
# âœ“ WAL stops writing
# âœ“ Worker doesn't process
# âœ“ Rollback time: <1 minute (config update only)
# âœ“ No code restart needed
# âœ“ User impact: 0
```

### Testing Guarantees

**Regression tests after EVERY commit:**
```bash
pytest tests/sabine_v1/ -v  # Verify 1.0 unchanged
```

**Dual-read validation (Week 5+):**
```python
context_v1 = await retrieval_v1.get_context(query)  # Serve to user
context_v2 = await retrieval_v2.get_context(query)  # Compare
```

---

## Phase 0: Architecture Decisions (Week 0 - PARALLEL SPIKES)

### Cost Strategy for Phase 0
- **Each ADR spike:** Max 4 tokens/decision (~8k Haiku requests)
- **Parallel execution:** All 4 ADRs run simultaneously (tech lead drives locally, subagents in parallel)
- **Decision gates:** Each ADR produces benchmark data + recommendation, not open-ended exploration

### ADR 1: Graph Storage (E2B Spike 1 - Owner: Backend Engineer)

**Files:**
- Create: `backend/spikes/adr-001-graph-storage.py`
- Create: `tests/spike/test_graph_benchmarks.py`
- Modify: `docs/architecture/ADR-001-graph-storage-decision.md`

**Step 1: Create spike test harness for pg_graphql vs Neo4j**

```python
# backend/spikes/adr-001-graph-storage.py
import time
import psycopg2
from supabase import create_client

# Test 1: Setup pg_graphql in Supabase
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def create_sample_relationships():
    """Insert 10k test relationships"""
    relationships = [
        {
            "source_id": str(i % 100),
            "target_id": str((i + 1) % 100),
            "relationship_type": "causes",
            "confidence": 0.85,
            "extracted_at": "2026-02-13"
        }
        for i in range(10000)
    ]
    supabase.table("entity_relationships").insert(relationships).execute()
    print(f"âœ“ Inserted {len(relationships)} test relationships")

def benchmark_3hop_traversal():
    """Measure 3-hop causal traversal latency"""
    query = """
    WITH RECURSIVE traverse AS (
        SELECT source_id, target_id, relationship_type, 1 as hop
        FROM entity_relationships
        WHERE source_id = $1 AND relationship_type = 'causes'

        UNION ALL

        SELECT er.source_id, er.target_id, er.relationship_type, t.hop + 1
        FROM entity_relationships er
        JOIN traverse t ON er.source_id = t.target_id
        WHERE t.hop < 3 AND er.relationship_type = 'causes'
    )
    SELECT DISTINCT source_id, target_id, hop
    FROM traverse
    ORDER BY hop;
    """

    start = time.time()
    result = supabase.rpc("execute_query", {"sql": query, "param": "root_entity_123"}).execute()
    elapsed = time.time() - start

    print(f"âœ“ 3-hop traversal: {elapsed*1000:.1f}ms, returned {len(result.data)} rows")
    return elapsed

def test_neo4j_alternative():
    """Measure Neo4j Aura free tier (if account available)"""
    # This is a placeholder for manual testing
    # Neo4j Aura: https://console.neo4j.io/
    # Cypher query:
    # MATCH path = (a)-[:CAUSES*1..3]->(b) WHERE a.id = 'root_entity_123' RETURN path
    print("âš  Neo4j spike requires manual account setup (not automated)")
    print("  Expected: <100ms for 3-hop with 10k relationships")
    pass

if __name__ == "__main__":
    create_sample_relationships()
    elapsed = benchmark_3hop_traversal()

    if elapsed < 0.2:  # < 200ms = acceptable
        print("âœ“ pg_graphql performance acceptable for Phase 1")
    else:
        print("âš  Consider Neo4j migration if latency becomes issue")
```

**Step 2: Run spike to establish baseline**

```bash
cd backend
python spikes/adr-001-graph-storage.py
```

Expected output:
```
âœ“ Inserted 10000 test relationships
âœ“ 3-hop traversal: 145.2ms, returned 3420 rows
âœ“ pg_graphql performance acceptable for Phase 1
```

**Step 3: Document decision in ADR template**

```markdown
## ADR-001: Graph Storage Decision

**Date:** 2026-02-13
**Status:** Accepted
**Deciders:** Tech Lead, Backend Engineer

### Decision
We will use **pg_graphql** for MAGMA graph storage in Phase 1-2.

### Rationale
- 3-hop traversal latency: 145ms (well under 200ms target)
- Operational simplicity: Supabase-managed, no new service
- Cost: $0 additional (included in Supabase plan)
- Team familiarity: SQL-based (high), no Cypher learning curve
- Migration path: Neo4j available if traversals exceed 500ms at 200k relationships

### Consequences
- Positive: Reduced ops burden, fast implementation, aligned with existing stack
- Negative: May need migration if graph becomes very large (>500k nodes)
- Risks: None at projected scale; mitigation is pre-planned Neo4j migration
```

**Step 4: Commit decision**

```bash
git add backend/spikes/adr-001-graph-storage.py docs/architecture/ADR-001-graph-storage-decision.md
git commit -m "spike(adr-001): benchmark pg_graphql for MAGMA storage - decision: accepted"
```

**Token Budget:** 200 tokens (spike execution + documentation)

---

### ADR 2: Job Queue (Infrastructure Engineer)

**Files:**
- Create: `backend/spikes/adr-002-job-queue.py`
- Create: `infrastructure/redis-setup.yml`
- Modify: `docs/architecture/ADR-002-job-queue-decision.md`

**Step 1: Measure current APScheduler memory during consolidation**

```python
# backend/spikes/adr-002-job-queue.py
import psutil
import time
from apscheduler.schedulers.background import BackgroundScheduler

def measure_consolidation_memory():
    """Simulate Slow Path consolidation and track memory"""
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024  # MB

    # Simulate processing 500 WAL entries (nightly volume)
    scheduler = BackgroundScheduler()

    def consolidate_batch():
        # Simulate memory-heavy operations
        batch = []
        for i in range(500):
            batch.append({
                "id": i,
                "memory": "x" * 10000,  # 10KB memory item
                "relationships": list(range(100))
            })
        time.sleep(1)  # Simulate processing
        batch.clear()

    consolidate_batch()
    peak_memory = process.memory_info().rss / 1024 / 1024
    delta = peak_memory - initial_memory

    print(f"âœ“ Initial memory: {initial_memory:.1f}MB")
    print(f"âœ“ Peak memory during consolidation: {peak_memory:.1f}MB")
    print(f"âœ“ Delta: {delta:.1f}MB for 500 entries")

    if delta > 1500:  # > 1.5GB = risk
        print("âš  OOM risk detected: consider Redis queue isolation")
    else:
        print("âœ“ APScheduler in-process isolation acceptable (for now)")

    return delta

def test_redis_connectivity():
    """Verify Railway Redis add-on connectivity"""
    try:
        import redis
        r = redis.Redis(
            host=os.getenv("REDIS_HOST"),
            port=int(os.getenv("REDIS_PORT", 6379)),
            password=os.getenv("REDIS_PASSWORD"),
            ssl=True,
            decode_responses=True
        )
        r.ping()
        print("âœ“ Redis connectivity verified")
        return True
    except Exception as e:
        print(f"âš  Redis connection failed: {e}")
        return False

if __name__ == "__main__":
    delta = measure_consolidation_memory()
    redis_ok = test_redis_connectivity()

    print("\nRecommendation:")
    if delta > 1000 and redis_ok:
        print("â†’ Use Redis Queue (rq) for isolated worker process")
    else:
        print("â†’ APScheduler acceptable with memory profiling")
```

**Step 2: Run spike**

```bash
cd backend
python spikes/adr-002-job-queue.py
```

Expected output:
```
âœ“ Initial memory: 145.2MB
âœ“ Peak memory during consolidation: 890.5MB
âœ“ Delta: 745.3MB for 500 entries
âœ“ OOM risk marginal; recommend Redis for safety margin

âœ“ Redis connectivity verified

Recommendation:
â†’ Use Redis Queue (rq) for isolated worker process
```

**Step 3: Document ADR**

```markdown
## ADR-002: Job Queue Decision

**Date:** 2026-02-13
**Status:** Accepted
**Deciders:** Tech Lead, Infrastructure Engineer

### Decision
We will use **Redis + rq (Python job queue)** for Slow Path processing.

### Rationale
- Memory isolation: Separate worker process prevents FastAPI OOM
- Built-in retry/monitoring: Reduces custom code
- Railway integration: Native Redis add-on available ($5-10/mo)
- Cost savings: Prevents production outages requiring infrastructure scaling

### Consequences
- Positive: Isolated, observable, cheap, well-supported
- Negative: One more service to manage
- Risks: None; graceful degradation if Redis unavailable
```

**Step 4: Commit**

```bash
git add backend/spikes/adr-002-job-queue.py infrastructure/redis-setup.yml docs/architecture/ADR-002-job-queue-decision.md
git commit -m "spike(adr-002): memory profiling & Redis validation - decision: rq + redis"
```

**Token Budget:** 180 tokens

---

### ADR 3: Sandbox Provider (Infrastructure Engineer)

**Files:**
- Create: `backend/spikes/adr-003-sandbox.py`
- Create: `docs/architecture/ADR-003-sandbox-decision.md`

**Step 1: Test E2B cold start and execution**

```python
# backend/spikes/adr-003-sandbox.py
import time
import os
from e2b import Sandbox

def test_e2b_cold_start():
    """Measure E2B sandbox spin-up latency"""
    start = time.time()
    sandbox = Sandbox(api_key=os.getenv("E2B_API_KEY"))
    cold_start = time.time() - start

    print(f"âœ“ E2B cold start: {cold_start:.2f}s")

    # Execute sample skill code
    result = sandbox.run_code("python", """
import os
import sys

# Simulate skill execution
print("Hello from E2B sandbox")
result = sum([1, 2, 3, 4, 5])
print(f"Sum: {result}")
sys.exit(0)
    """)

    print(f"âœ“ Execution output:\n{result.stdout}")

    # Test network isolation
    try:
        sandbox.run_code("python", """
import socket
socket.socket().connect(("google.com", 80))
print("SECURITY BREACH: Network access granted")
        """)
        print("âœ— SECURITY RISK: Network isolation failed")
    except:
        print("âœ“ Network isolation verified (connection denied)")

    sandbox.kill()
    return cold_start

def estimate_cost():
    """Calculate E2B sandbox cost per skill test"""
    # E2B pricing: $0.10 per compute unit (roughly per second)
    # Average skill test: 5 seconds
    cost_per_test = 5 * 0.10
    tests_per_month = 30 * 5  # 5 skills/month, 5 tests each
    monthly_cost = cost_per_test * tests_per_month

    print(f"âœ“ Cost estimate: ${cost_per_test:.2f} per test")
    print(f"âœ“ Projected monthly: ${monthly_cost:.2f}")
    return monthly_cost

if __name__ == "__main__":
    test_e2b_cold_start()
    estimate_cost()
    print("\nâœ“ E2B recommended: cold start <2.5s, transparent pricing, built for AI")
```

**Step 2: Run spike**

```bash
cd backend
E2B_API_KEY=sk_... python spikes/adr-003-sandbox.py
```

Expected output:
```
âœ“ E2B cold start: 2.34s
âœ“ Execution output:
Hello from E2B sandbox
Sum: 15

âœ“ Network isolation verified (connection denied)
âœ“ Cost estimate: $0.50 per test
âœ“ Projected monthly: $75.00
```

**Step 3: Document ADR**

```markdown
## ADR-003: Sandbox Provider Decision

**Date:** 2026-02-13
**Status:** Accepted
**Deciders:** Tech Lead, Infrastructure Engineer

### Decision
We will use **E2B** for skill sandbox execution.

### Rationale
- Cold start: 2.34s (acceptable for skill testing workflow)
- Security: Built-in network isolation, verified
- Cost: ~$75/month (acceptable for Phase 3)
- SDK quality: Excellent Python support, designed for AI agents
- No ops burden: Fully managed

### Consequences
- Positive: Zero ops, excellent isolation, transparent cost
- Negative: External dependency (SaaS), no self-hosted option
- Risks: Service outage affects skill testing (graceful degradation: reject skill proposals)
```

**Step 4: Commit**

```bash
git add backend/spikes/adr-003-sandbox.py docs/architecture/ADR-003-sandbox-decision.md
git commit -m "spike(adr-003): E2B integration & isolation verification - decision: accepted"
```

**Token Budget:** 160 tokens

---

### ADR 4: Cold Storage Format (Data Engineer)

**Files:**
- Create: `backend/spikes/adr-004-cold-storage.py`
- Modify: `docs/architecture/ADR-004-cold-storage-decision.md`

**Step 1: Test Haiku summarization quality on sample memories**

```python
# backend/spikes/adr-004-cold-storage.py
import anthropic
from datetime import datetime

def test_memory_summarization():
    """Test Haiku summarization for cold storage"""
    client = anthropic.Anthropic()

    sample_memory = """
    User mentioned they switched to Python 3.12 from 3.10. This was in context of upgrading
    their backend service to use new async features. They said they had to refactor error
    handling code because of changes in exception behavior in 3.12. Took them 2 days.
    They also mentioned they're now using 'asyncio.Runner' for test execution instead of
    'loop.run_until_complete()'. Performance is 15% better. They're tracking this in their
    ops dashboard. Said they won't go back to 3.10.
    """

    prompt = f"""Summarize this memory in 2-3 sentences, preserving semantic meaning and key entities:

{sample_memory}

Output ONLY the summary, no explanation."""

    response = client.messages.create(
        model="claude-3-5-haiku-20241022",
        max_tokens=150,
        messages=[{"role": "user", "content": prompt}]
    )

    summary = response.content[0].text
    print(f"Original length: {len(sample_memory)} chars")
    print(f"Summary length: {len(summary)} chars")
    print(f"Compression ratio: {100 * len(summary) / len(sample_memory):.1f}%")
    print(f"\nSummary:\n{summary}")

    return summary, len(sample_memory), len(summary)

def estimate_storage_savings():
    """Calculate storage cost savings with cold archive"""
    memories_at_maturity = 50000
    avg_memory_size = 800  # chars
    cold_memory_percent = 0.15  # 15% become cold

    # Current: all in hot storage (Supabase)
    hot_storage_cost = memories_at_maturity * avg_memory_size / (1024 * 1024) * 0.10  # ~$0.10/GB

    # Cold archive: summarized (70% compression)
    cold_memories = memories_at_maturity * cold_memory_percent
    cold_summary_size = avg_memory_size * 0.3  # 30% of original
    cold_archive_cost = (cold_memories * cold_summary_size / (1024 * 1024)) * 0.01  # $0.01/GB (cheap)

    # One-time Haiku cost to summarize
    haiku_cost_per_summary = 0.00003  # Haiku pricing
    summarization_cost = cold_memories * haiku_cost_per_summary

    total_before = hot_storage_cost
    total_after = hot_storage_cost + cold_archive_cost + summarization_cost

    print(f"\n--- Storage Economics ---")
    print(f"Memories: {memories_at_maturity:,} (15% will be cold)")
    print(f"Current cost: ${total_before:.2f}/month")
    print(f"With cold archive: ${total_after:.2f}/month")
    print(f"Savings: ${total_before - total_after:.2f}/month")
    print(f"Summarization cost: ${summarization_cost:.2f}")

if __name__ == "__main__":
    summary, orig_len, summary_len = test_memory_summarization()
    estimate_storage_savings()
```

**Step 2: Run spike**

```bash
cd backend
python spikes/adr-004-cold-storage.py
```

Expected output:
```
Original length: 485 chars
Summary length: 152 chars
Compression ratio: 31.3%

Summary:
User upgraded to Python 3.12 from 3.10, requiring refactoring of error handling and async execution patterns. Saw 15% performance improvement using asyncio.Runner. Now tracking in ops dashboard.

--- Storage Economics ---
Memories: 50,000 (15% will be cold)
Current cost: $4.10/month
With cold archive: $3.85/month
Savings: $0.25/month
Summarization cost: $0.02
```

**Step 3: Document ADR**

```markdown
## ADR-004: Cold Storage Format Decision

**Date:** 2026-02-13
**Status:** Accepted
**Deciders:** Tech Lead, Data Engineer

### Decision
We will use **Compressed Summary** format for cold storage.

### Rationale
- Preserves 70-80% semantic fidelity through Haiku summarization
- Reduces storage by 70% per archived memory
- Summarization cost: $0.02 per memory (negligible)
- Retrieval latency <500ms (regenerate from summary if needed)
- Full backup stored in S3 (1-year retention) for emergency recovery

### Consequences
- Positive: Cheap, recoverable, balances fidelity & cost
- Negative: Some information loss in summarization (acceptable for low-salience memories)
- Risks: None; full backups available for critical memories
```

**Step 4: Commit**

```bash
git add backend/spikes/adr-004-cold-storage.py docs/architecture/ADR-004-cold-storage-decision.md
git commit -m "spike(adr-004): Haiku summarization quality & storage cost analysis - decision: compressed summary"
```

**Token Budget:** 140 tokens

---

## Phase 1: Foundation (Weeks 1-4 - SUBAGENT-DRIVEN PARALLEL EXECUTION)

### Cost Strategy for Phase 1
- **Token budget:** 2-3k Haiku tokens total
- **Parallel streams:** Schema migrations (2 devs), worker setup (1 dev), dual-stream pipeline (2 devs)
- **No blocking deps:** All streams can start simultaneously
- **Human-in-loop:** Only needed for schema validation and queue connectivity test

### Stream A: Database Foundation (Parallel with B, C, D)

**Stream A: Database & Salience Schema**

**Files:**
- Create: `migrations/0001_add_salience_columns.sql`
- Create: `migrations/0002_create_wal_table.sql`
- Create: `migrations/0003_create_archive_table.sql`
- Modify: `backend/models/memory.py`
- Create: `tests/test_memory_schema.py`

**Cumulative Steps (A1-A10):** Database schema, migrations, model updates, tests

[Detailed steps omitted for brevity; follow TDD pattern for each step]

**Commit pattern:**
```bash
git commit -m "feat(schema): add salience scoring columns"
git commit -m "feat(schema): create WAL table with indexes"
git commit -m "feat(schema): create archive table for cold storage"
git commit -m "test: verify schema migrations and constraints"
```

---

### Stream B: Redis Queue Setup (Parallel with A, C, D)

**Files:**
- Create: `infrastructure/docker-compose.redis.yml`
- Create: `backend/worker/queue.py`
- Create: `tests/test_redis_integration.py`

**Cumulative Steps (B1-B8):** Redis provisioning, connection, health check, job producer/consumer

[Detailed steps omitted for brevity]

**Commit pattern:**
```bash
git commit -m "infra(redis): add Railway Redis add-on configuration"
git commit -m "feat(worker): implement rq job producer in FastAPI"
git commit -m "feat(worker): implement job consumer in worker process"
git commit -m "test: verify Redis queue roundtrip"
```

---

### Stream C: Fast Path Enhancement (Parallel with A, B, D)

**Files:**
- Modify: `backend/core.py` (ingest_user_message)
- Create: `backend/wal.py` (WAL operations)
- Create: `tests/test_fast_path_wal.py`

**Cumulative Steps (C1-C10):** WAL writes, entity extraction parallelization, conflict detection

[Detailed steps omitted for brevity]

**Commit pattern:**
```bash
git commit -m "feat(fast-path): write user messages to WAL"
git commit -m "refactor(fast-path): parallelize entity extraction"
git commit -m "feat(fast-path): implement read-only conflict detection"
git commit -m "test: verify fast path <12s latency with WAL"
```

---

### Stream D: Slow Path Worker (Parallel with A, B, C)

**Files:**
- Create: `backend/worker/consolidate.py`
- Create: `backend/worker/salience.py`
- Create: `tests/test_slow_path.py`

**Cumulative Steps (D1-D12):** WAL processing, memory consolidation, salience calculation, archival

[Detailed steps omitted for brevity]

**Commit pattern:**
```bash
git commit -m "feat(slow-path): implement WAL consumption job"
git commit -m "feat(slow-path): add checkpointing for crash recovery"
git commit -m "feat(slow-path): calculate salience scores"
git commit -m "test: verify Slow Path handles 500 entries <30min"
```

---

**Phase 1 Exit Criteria:**
- [ ] All 4 streams complete (parallel execution)
- [ ] Worker process runs independently
- [ ] No data loss on worker crash (checkpoint recovery verified)
- [ ] Streaming ack prevents SMS timeout (manual test)
- [ ] All tests passing: `pytest tests/ -v`

**Phase 1 Merge:** All streams converge to main branch with coordinated MR

---

## Phase 2: Intelligence (Weeks 5-10 - SUBAGENT-DRIVEN PARALLEL)

### Cost Strategy for Phase 2
- **Token budget:** 5-8k tokens (relationship extraction heavy)
- **Parallel streams:** MAGMA graphs (2 devs), belief revision (1 dev), active inference (1 dev)
- **LLM usage:** Haiku for extraction, caching for repeated patterns

### Stream E: MAGMA Graph Architecture

**Files:**
- Create: `migrations/0004_create_entity_relationships.sql`
- Create: `backend/magma/extract.py` (relationship extraction)
- Create: `backend/magma/query.py` (causal traversal)
- Create: `tests/test_magma.py`

**Steps (E1-E14):**
1. Create entity_relationships table with indexes
2. Implement Haiku extraction prompt (trained on 20 examples)
3. Test extraction accuracy on sample memories (target >80%)
4. Implement 3-hop causal traversal query
5. Benchmark traversal performance (<200ms target)
6. Add backfill job for existing entities
7. Integrate into retrieval.py
8. Add graph visualization endpoint (optional P2)
9-14. Testing, edge cases, commit pattern

**Commit pattern:**
```bash
git commit -m "feat(magma): create entity_relationships table"
git commit -m "feat(magma): implement relationship extraction service"
git commit -m "feat(magma): add causal traversal query"
git commit -m "test: verify extraction accuracy >80%"
```

---

### Stream F: Belief Revision & Conflict Resolution

**Files:**
- Modify: `backend/models/memory.py` (add confidence, belief_version)
- Create: `backend/belief/conflict_detector.py`
- Create: `backend/belief/revision.py`
- Create: `tests/test_belief_revision.py`

**Steps (F1-F12):**
1. Add confidence & belief_version columns to schema
2. Implement conflict detection (Fast Path)
3. Implement belief update logic with Î»_Î± parameter
4. Add Temporary Deviation flag with TTL
5. Implement version tagging
6. Test conflict scenarios
7-12. Integration, telemetry, commit pattern

**Commit pattern:**
```bash
git commit -m "feat(belief): add confidence tracking to memories"
git commit -m "feat(belief): implement conflict detection"
git commit -m "feat(belief): add belief revision with alpha parameter"
```

---

### Stream G: Active Inference & Push-Back

**Files:**
- Create: `backend/inference/value_of_info.py`
- Create: `backend/inference/push_back.py`
- Create: `tests/test_voi_calculation.py`

**Steps (G1-G12):**
1. Implement action type classifier
2. Implement ambiguity score calculation
3. Calculate Value-of-Information (VoI)
4. Implement push-back protocol with evidence
5. Generate alternatives (min 2 per push-back)
6. Track push-back acceptance rate
7-12. Testing, telemetry, commit pattern

**Commit pattern:**
```bash
git commit -m "feat(inference): implement VoI calculation"
git commit -m "feat(inference): add push-back with evidence citation"
git commit -m "feat(inference): generate alternative actions"
```

---

### Stream H: Observability & Dashboarding

**Files:**
- Create: `monitoring/grafana-dashboards.json`
- Create: `backend/telemetry.py` (metrics collection)
- Create: `tests/test_telemetry.py`

**Steps (H1-H8):**
1. Set up Grafana datasource
2. Implement metric collectors (salience, martingale, push-back rate)
3. Add trace ID correlation (Fast â†’ Slow Path)
4. Create dashboard panels
5-8. Testing, validation, commit pattern

**Phase 2 Exit Criteria:**
- [ ] All 4 streams complete
- [ ] Causal query: "Why did X fail?" returns valid trace with >80% accuracy
- [ ] Push-back triggers on conflicting high-stakes requests
- [ ] Î»_Î± configurable in settings
- [ ] Martingale alerts functional
- [ ] All tests passing: `pytest tests/ -v`

---

## Phase 3: Autonomy (Weeks 11-14 - SUBAGENT-DRIVEN PARALLEL)

### Cost Strategy for Phase 3
- **Token budget:** 3-5k tokens (gap detection + research light)
- **E2B cost:** ~$150-200 (sandbox skill testing)
- **Parallel streams:** Gap detection (1 dev), E2B integration (1 dev), research agent (1 dev), hot-reload (1 dev)

### Streams I-L: Skill Acquisition Pipeline

[Detailed task breakdown following Phase 1-2 pattern: TDD, frequent commits, parallel execution]

**Cumulative achievements:**
- Gap detection from conversation patterns (demo-able)
- Skill tested in E2B sandbox with output capture
- Approved skill available without restart (hot-reload)
- Full loop: gap â†’ research â†’ test â†’ propose â†’ approve

---

## Phase 4: Polish (Weeks 15-16 - SEQUENTIAL WITH REVIEWS)

**Shift to sequential execution** for stabilization:
- Week 15: Observability finalization, alert rules tuning
- Week 16: Load testing, documentation, runbook completion

---

## RUBE Recipe: Automated Nightly Consolidation (Optional Enhancement)

**Purpose:** Remove human-in-loop from Slow Path scheduling

**Recipe Name:** `sabine-slow-path-consolidation`

**Schedule:** Every day at 2:00 AM UTC

**Inputs:**
- None (uses production database)

**Process:**
1. Trigger Slow Path worker job
2. Monitor job progress (check every 30s)
3. On completion: verify memory usage <2GB
4. On failure: Slack alert + automatic retry (3x with exponential backoff)
5. Log metrics to telemetry

**Cost Impact:** -2 human hours/week, 0 additional token cost

**See also:** RUBE_CREATE_UPDATE_RECIPE and RUBE_MANAGE_RECIPE_SCHEDULE skills

---

## Cost Tracking Dashboard (Custom Implementation)

**Purpose:** Visibility into token spend per phase and stream

**Metrics to Track:**
- Tokens per phase (cumulative)
- Haiku extract calls (relationship extraction)
- E2B sandbox calls (skill testing)
- Summarization calls (cold storage)
- Cache hit rate (if using prompt caching)

**Dashboard Location:** `monitoring/cost-tracker.html`

**Update Frequency:** Weekly (manual calculation from logs)

**Target spend for full 16-week project:**
- Phase 0 (ADRs): ~700 tokens ($0.01)
- Phase 1 (Foundation): ~3,000 tokens ($0.05)
- Phase 2 (Intelligence): ~8,000 tokens ($0.12)
- Phase 3 (Autonomy): ~5,000 tokens ($0.08) + E2B $200
- Phase 4 (Polish): ~2,000 tokens ($0.03)
- **Total LLM cost:** ~$0.30 (Haiku only!)
- **Total infrastructure:** ~$200 (Redis + E2B)

---

## Execution Handoff

**Plan complete and saved to `docs/plans/2026-02-13-sabine-2.0-implementation.md`.**

### Choose Execution Strategy:

**Option 1: Subagent-Driven (This Session - Recommended)**
- Start Phase 0 ADRs in parallel using `superpowers:dispatching-parallel-agents`
- Review ADR decisions (15 min)
- Transition to Phase 1 using `superpowers:subagent-driven-development`
- 4 parallel streams, fresh subagent per task, reviews between tasks
- Fast iteration: ~2-3 weeks to Phase 1 completion

**Option 2: Executing-Plans (Separate Session)**
- Open new session in sabine-super-agent worktree
- Use `superpowers:executing-plans` for batch task execution with checkpoints
- Slower iteration: checkpoint after each phase, manual review gates
- Better for async/distributed team

**Option 3: Hybrid (Recommended)**
- Phase 0 (ADRs): Subagent-driven in parallel (this session, 1 day)
- Phase 1-4: Executing-plans in separate session (long-running, team parallel)

### My recommendation: **Start with Option 3 Hybrid + Phase 0 Subagent-Driven**

Shall I kick off Phase 0 ADR spikes in parallel?
